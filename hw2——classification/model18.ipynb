{"cells":[{"cell_type":"markdown","metadata":{"id":"OYlaRwNu7ojq"},"source":["# **Homework 2-1 Phoneme Classification**"]},{"cell_type":"markdown","metadata":{"id":"emUd7uS7crTz"},"source":["## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n","The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n","\n","This homework is a multiclass classification task, \n","we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n","\n","link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"]},{"cell_type":"markdown","metadata":{"id":"KVUGfWTo7_Oj"},"source":["## Download Data\n","Download data from google drive, then unzip it.\n","\n","You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.\u003cbr\u003e\u003cbr\u003e\n","`timit_11/`\n","- `train_11.npy`: training data\u003cbr\u003e\n","- `train_label_11.npy`: training label\u003cbr\u003e\n","- `test_11.npy`:  testing data\u003cbr\u003e\u003cbr\u003e\n","\n","**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OzkiMEcC3Foq"},"outputs":[],"source":["#!gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n","#!unzip data.zip\n","#!ls "]},{"cell_type":"markdown","metadata":{"id":"_L_4anls8Drv"},"source":["## Preparing Data\n","Load the training and testing data from the `.npy` file (NumPy array)."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8959087,"status":"ok","timestamp":1617214360622,"user":{"displayName":"MLTEST5 NTU","photoUrl":"","userId":"01899244368686744251"},"user_tz":-480},"id":"lYN6oNZi_xze","outputId":"ae58cdc5-3cb4-488a-9d78-d98dcec35db9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJjLT8em-y9G","outputId":"392235ec-0786-4a88-a0d9-901bf1833f11"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading data ...\n"]}],"source":["import numpy as np\n","\n","print('Loading data ...')\n","\n","data_root='/content/drive/MyDrive/timit_11/'\n","train = np.load(data_root + 'train_11.npy')\n","train_label = np.load(data_root + 'train_label_11.npy')\n","test = np.load(data_root + 'test_11.npy')\n","\n","print('Size of training data: {}'.format(train.shape))\n","print('Size of testing data: {}'.format(test.shape))"]},{"cell_type":"markdown","metadata":{"id":"us5XW_x6udZQ"},"source":["## Create Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Fjf5EcmJtf4e"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","\n","class TIMITDataset(Dataset):\n","    def __init__(self, X, y=None):\n","        self.data = torch.from_numpy(X).float()\n","        if y is not None:\n","            y = y.astype(np.int)\n","            self.label = torch.LongTensor(y)\n","        else:\n","            self.label = None\n","\n","    def __getitem__(self, idx):\n","        if self.label is not None:\n","            return self.data[idx], self.label[idx]\n","        else:\n","            return self.data[idx]\n","\n","    def __len__(self):\n","        return len(self.data)\n"]},{"cell_type":"markdown","metadata":{"id":"otIC6WhGeh9v"},"source":["Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sYqi_lAuvC59","outputId":"fe467d9e-2da3-4730-a920-99299450ac29"},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of training set: (1205333, 429)\n","Size of validation set: (24599, 429)\n"]}],"source":["VAL_RATIO = 0.02\n","\n","percent = int(train.shape[0] * (1 - VAL_RATIO))\n","train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n","print('Size of training set: {}'.format(train_x.shape))\n","print('Size of validation set: {}'.format(val_x.shape))"]},{"cell_type":"markdown","metadata":{"id":"nbCfclUIgMTX"},"source":["Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RUCbQvqJurYc"},"outputs":[],"source":["BATCH_SIZE = 32\n","\n","from torch.utils.data import DataLoader\n","\n","train_set = TIMITDataset(train_x, train_y)\n","val_set = TIMITDataset(val_x, val_y)\n","train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n","val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"_SY7X0lUgb50"},"source":["Cleanup the unneeded variables to save memory.\u003cbr\u003e\n","\n","**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later\u003cbr\u003ethe data size is quite huge, so be aware of memory usage in colab**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"y8rzkGraeYeN","outputId":"90b50b7c-b03b-4865-9462-8982443378c0"},"outputs":[{"data":{"text/plain":["153"]},"execution_count":0,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["import gc\n","\n","del train, train_label, train_x, train_y, val_x, val_y\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"IRqKNvNZwe3V"},"source":["## Create Model"]},{"cell_type":"markdown","metadata":{"id":"FYr1ng5fh9pA"},"source":["Define model architecture, you are encouraged to change and experiment with the model architecture."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lbZrwT6Ny0XL"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class Classifier(nn.Module):\n","    def __init__(self):\n","        super(Classifier, self).__init__()\n","        self.layer1 = nn.Linear(429, 2048)\n","        self.layer2 = nn.Linear(2048, 1024)\n","        self.layer3 = nn.Linear(1024, 512)\n","        self.out = nn.Linear(512, 39)\n","\n","        \"\"\"\n","        self.layer1 = nn.Linear(429, 1024)\n","        self.layer2 = nn.Linear(1024, 512)\n","        self.layer3 = nn.Linear(512, 128)\n","        self.out = nn.Linear(128, 39) \n","        \"\"\"\n","\n","        self.act_fn = nn.ReLU()\n","        self.BN0 = nn.BatchNorm1d(429)\n","        self.BN1 = nn.BatchNorm1d(2048)\n","        self.BN2 = nn.BatchNorm1d(1024)\n","        self.BN3 = nn.BatchNorm1d(512)\n","        self.drop_out1 = nn.Dropout(0.3)\n","        self.drop_out2 = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        x = self.BN0(x)\n","\n","        x = self.layer1(x)\n","        x = self.BN1(x)\n","        x = self.act_fn(x)\n","        x = self.drop_out1(x)\n","\n","        x = self.layer2(x)\n","        x = self.BN2(x)\n","        x = self.act_fn(x)\n","        x = self.drop_out1(x)\n","\n","        x = self.layer3(x)\n","        x = self.BN3(x)\n","        x = self.act_fn(x)\n","        x = self.drop_out1(x)\n","\n","        x = self.out(x)\n","        \n","        return x"]},{"cell_type":"markdown","metadata":{"id":"VRYciXZvPbYh"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"y114Vmm3Ja6o"},"outputs":[],"source":["#check device\n","def get_device():\n","  return 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"sEX-yjHjhGuH"},"source":["Fix random seeds for reproducibility."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"88xPiUnm0tAd"},"outputs":[],"source":["# fix random seed\n","def same_seeds(seed):\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)  \n","    np.random.seed(seed)  \n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{"id":"KbBcBXkSp6RA"},"source":["Feel free to change the training parameters here."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QTp3ZXg1yO9Y","outputId":"96e6ceb4-926e-4101-bf8b-0a49259162e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["DEVICE: cuda\n"]}],"source":["# fix random seed for reproducibility\n","same_seeds(0)\n","\n","# get device \n","device = get_device()\n","print(f'DEVICE: {device}')\n","\n","# training parameters\n","num_epoch = 35               # number of training epoch\n","learning_rate = 0.0001       # learning rate\n","\n","# the path where checkpoint saved\n","model_saved_path = \"/content/drive/MyDrive/hw2-18/tmp_model/model.ckpt\"\n","model_path = './model.ckpt'\n","\n","# create model, define a loss function, and optimizer\n","model = Classifier().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001, amsgrad=True)\n","\n","# checkpoint mode\n","checkpoint = torch.load(model_saved_path)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","epoch = checkpoint['epoch']\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9053842,"status":"ok","timestamp":1617214336883,"user":{"displayName":"MLTEST5 NTU","photoUrl":"","userId":"01899244368686744251"},"user_tz":-480},"id":"CdMWsBs7zzNs","outputId":"d03eb1ac-15aa-4eeb-8024-6e8fe5465bc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[001/035] Train Acc: 0.737054 Loss: 0.786808 | Val Acc: 0.757348 loss: 0.734740\n","saving model with acc 0.757\n","[002/035] Train Acc: 0.735929 Loss: 0.788970 | Val Acc: 0.756250 loss: 0.735628\n","[003/035] Train Acc: 0.736424 Loss: 0.789195 | Val Acc: 0.758893 loss: 0.734381\n","saving model with acc 0.759\n","[004/035] Train Acc: 0.736556 Loss: 0.787840 | Val Acc: 0.753974 loss: 0.744289\n","[005/035] Train Acc: 0.736361 Loss: 0.787092 | Val Acc: 0.759909 loss: 0.739097\n","saving model with acc 0.760\n","[006/035] Train Acc: 0.736698 Loss: 0.786624 | Val Acc: 0.760112 loss: 0.731823\n","saving model with acc 0.760\n","[007/035] Train Acc: 0.737181 Loss: 0.786278 | Val Acc: 0.752876 loss: 0.739593\n","[008/035] Train Acc: 0.737437 Loss: 0.784647 | Val Acc: 0.755315 loss: 0.741791\n","[009/035] Train Acc: 0.736939 Loss: 0.784588 | Val Acc: 0.754340 loss: 0.736336\n","[010/035] Train Acc: 0.736775 Loss: 0.784626 | Val Acc: 0.754055 loss: 0.739687\n","[011/035] Train Acc: 0.737366 Loss: 0.783323 | Val Acc: 0.760072 loss: 0.727397\n","[012/035] Train Acc: 0.737676 Loss: 0.783204 | Val Acc: 0.759787 loss: 0.734673\n","[013/035] Train Acc: 0.738218 Loss: 0.781993 | Val Acc: 0.756047 loss: 0.739139\n","[014/035] Train Acc: 0.738206 Loss: 0.781014 | Val Acc: 0.758039 loss: 0.733772\n","[015/035] Train Acc: 0.738628 Loss: 0.780473 | Val Acc: 0.758039 loss: 0.736521\n","[016/035] Train Acc: 0.738271 Loss: 0.780684 | Val Acc: 0.755640 loss: 0.736599\n","[017/035] Train Acc: 0.738729 Loss: 0.779773 | Val Acc: 0.757307 loss: 0.732726\n","[018/035] Train Acc: 0.738962 Loss: 0.779320 | Val Acc: 0.754096 loss: 0.736959\n","[019/035] Train Acc: 0.738497 Loss: 0.779583 | Val Acc: 0.757389 loss: 0.731209\n","[020/035] Train Acc: 0.739362 Loss: 0.777557 | Val Acc: 0.756779 loss: 0.736444\n","[021/035] Train Acc: 0.739276 Loss: 0.778104 | Val Acc: 0.759218 loss: 0.730801\n","[022/035] Train Acc: 0.739214 Loss: 0.776617 | Val Acc: 0.757836 loss: 0.735855\n","[023/035] Train Acc: 0.739265 Loss: 0.777091 | Val Acc: 0.757673 loss: 0.727001\n","[024/035] Train Acc: 0.740260 Loss: 0.776099 | Val Acc: 0.756128 loss: 0.732660\n","[025/035] Train Acc: 0.739762 Loss: 0.775759 | Val Acc: 0.754909 loss: 0.738722\n","[026/035] Train Acc: 0.739499 Loss: 0.776019 | Val Acc: 0.759502 loss: 0.729244\n","[027/035] Train Acc: 0.739591 Loss: 0.775719 | Val Acc: 0.758324 loss: 0.731248\n","[028/035] Train Acc: 0.740214 Loss: 0.773978 | Val Acc: 0.761007 loss: 0.717213\n","saving model with acc 0.761\n","[029/035] Train Acc: 0.740458 Loss: 0.773678 | Val Acc: 0.758486 loss: 0.735168\n","[030/035] Train Acc: 0.740466 Loss: 0.772435 | Val Acc: 0.761129 loss: 0.728868\n","saving model with acc 0.761\n","[031/035] Train Acc: 0.740706 Loss: 0.772222 | Val Acc: 0.753892 loss: 0.736806\n","[032/035] Train Acc: 0.740898 Loss: 0.771766 | Val Acc: 0.761291 loss: 0.726507\n","saving model with acc 0.761\n","[033/035] Train Acc: 0.741435 Loss: 0.771060 | Val Acc: 0.758445 loss: 0.733293\n","[034/035] Train Acc: 0.740464 Loss: 0.772971 | Val Acc: 0.755437 loss: 0.741701\n","[035/035] Train Acc: 0.741599 Loss: 0.769627 | Val Acc: 0.760885 loss: 0.723895\n"]}],"source":["# start training\n","\n","best_acc = checkpoint['best_acc']\n","best_acc = 0.0\n","for epoch in range(num_epoch):\n","    train_acc = 0.0\n","    train_loss = 0.0\n","    val_acc = 0.0\n","    val_loss = 0.0\n","\n","    # training\n","    model.train() # set the model to training mode\n","    for i, data in enumerate(train_loader):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad() \n","        outputs = model(inputs) \n","        batch_loss = criterion(outputs, labels)\n","        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n","        batch_loss.backward() \n","        optimizer.step() \n","\n","        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n","        train_loss += batch_loss.item()\n","\n","    # validation\n","    if len(val_set) \u003e 0:\n","        model.eval() # set the model to evaluation mode\n","        with torch.no_grad():\n","            for i, data in enumerate(val_loader):\n","                inputs, labels = data\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                batch_loss = criterion(outputs, labels) \n","                _, val_pred = torch.max(outputs, 1) \n","            \n","                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n","                val_loss += batch_loss.item()\n","\n","            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n","                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n","            ))\n","\n","            # if the model improves, save a checkpoint at this epoch\n","            if val_acc \u003e best_acc:\n","                best_acc = val_acc\n","                torch.save(model.state_dict(), model_path)\n","                torch.save({\n","                              'epoch': epoch,\n","                              'model_state_dict': model.state_dict(),\n","                              'optimizer_state_dict': optimizer.state_dict(),\n","                              'train_loss': train_loss,\n","                              'train_acc': train_acc,\n","                              'val_loss': val_loss,\n","                              'val_acc': val_acc,\n","                              'best_acc': best_acc \n","                            }, model_path)\n","                torch.save(model.state_dict(), model_saved_path)\n","                torch.save({\n","                              'epoch': epoch,\n","                              'model_state_dict': model.state_dict(),\n","                              'optimizer_state_dict': optimizer.state_dict(),\n","                              'train_loss': train_loss,\n","                              'train_acc': train_acc,\n","                              'val_loss': val_loss,\n","                              'val_acc': val_acc,\n","                              'best_acc': best_acc \n","                            }, model_saved_path)\n","                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n","\n","    else:\n","        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n","            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n","        ))\n","\n","# if not validating, save the last epoch\n","if len(val_set) == 0:\n","    torch.save(model.state_dict(), model_path)\n","    print('saving model at last epoch')\n"]},{"cell_type":"markdown","metadata":{"id":"1Hi7jTn3PX-m"},"source":["## Testing"]},{"cell_type":"markdown","metadata":{"id":"NfUECMFCn5VG"},"source":["Create a testing dataset, and load model from the saved checkpoint."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":498,"status":"ok","timestamp":1617214336900,"user":{"displayName":"MLTEST5 NTU","photoUrl":"","userId":"01899244368686744251"},"user_tz":-480},"id":"1PKjtAScPWtr","outputId":"1f1c3935-f1d7-4777-cd55-01ed40ebab8a"},"outputs":[{"data":{"text/plain":["\u003cAll keys matched successfully\u003e"]},"execution_count":13,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# create testing dataset\n","test_set = TIMITDataset(test, None)\n","test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n","\n","# create model and load weights from checkpoint\n","model = Classifier().to(device)\n","checkpoint = torch.load(model_saved_path)\n","model.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"markdown","metadata":{"id":"940TtCCdoYd0"},"source":["Make prediction."]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":14926,"status":"ok","timestamp":1617214351537,"user":{"displayName":"MLTEST5 NTU","photoUrl":"","userId":"01899244368686744251"},"user_tz":-480},"id":"84HU5GGjPqR0"},"outputs":[],"source":["predict = []\n","model.eval() # set the model to evaluation mode\n","with torch.no_grad():\n","    for i, data in enumerate(test_loader):\n","        inputs = data\n","        inputs = inputs.to(device)\n","        outputs = model(inputs)\n","        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n","\n","        for y in test_pred.cpu().numpy():\n","            predict.append(y)"]},{"cell_type":"markdown","metadata":{"id":"AWDf_C-omElb"},"source":["Write prediction to a CSV file.\n","\n","After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuljYSPHcZir"},"outputs":[],"source":["with open('prediction.csv', 'w') as f:\n","    f.write('Id,Class\\n')\n","    for i, y in enumerate(predict):\n","        f.write('{},{}\\n'.format(i, y))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zd6r4i8kDGUZ"},"outputs":[],"source":["torch.save(model.state_dict(), \"/content/drive/MyDrive/model/model.ckpt\")\n","torch.save({\n","                              'epoch': epoch,\n","                              'model_state_dict': model.state_dict(),\n","                              'optimizer_state_dict': optimizer.state_dict(),\n","                              'train_loss': train_loss,\n","                              'train_acc': train_acc,\n","                              'val_loss': val_loss,\n","                              'val_acc': val_acc,\n","                              'best_acc': best_acc \n","                            }, \"/content/drive/MyDrive/model/model.ckpt\")"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":40,"status":"ok","timestamp":1617214360621,"user":{"displayName":"MLTEST5 NTU","photoUrl":"","userId":"01899244368686744251"},"user_tz":-480},"id":"Hs9YPQviC4uW"},"outputs":[],"source":["%cp /content/prediction.csv /content/drive/MyDrive/output/prediction.csv\n","%cp /content/drive/MyDrive/tmp_model/model.ckpt /content/drive/MyDrive/model/model.ckpt"]},{"cell_type":"markdown","metadata":{"id":"qr2-QF1AFbW4"},"source":["\n","# Testing log\n","---\n","# 1. Val = 0.2, Batch = 64, Sigmoid, Adam (lr = 0.0001, num_epoch = 20)\n","\n","## [Upload: 0.68717]\n","##### [017/020] Train Acc: 0.779358 Loss: 0.674498 | Val Acc: 0.702492 loss: 0.943941, saving model with acc 0.702 \n","\n","---\n","# 2. Val = 0.2, Batch = 32, ReLU, Adadelta (num_epoch = 20)\n","# (From 1: 減小B, 改ReLU, 改Adadelta)\n","#### [011/020] Train Acc: 0.425888 Loss: 2.169479 | Val Acc: 0.445255 loss: 2.100470, saving model with acc 0.445\n","\n","---\n","# 3. Val = 0.1, Batch = 64, ReLU, Adam (lr = 0.0001, num_epoch = 20)\n","# (From 1: 改Val, 改ReLU)\n","\n","## [Upload: 0.70362]\n","\n","#### [007/020] Train Acc: 0.781556 Loss: 0.650488 | Val Acc: 0.719173 loss: 0.874977, saving model with acc 0.719\n","\n","---\n","# 4. Val = 0.1, Batch = 64, ReLU, Adadelta (num_epoch = 20)\n","\n","# (From 1: 改Val, 改ReLU, 改Adadelta)\n","#### [001/020] Train Acc: 0.586574 Loss: 1.439755 | Val Acc: 0.606330 loss: 1.429865, saving model with acc 0.606\n","\n","\n","---\n","# 5. Val = 0.1, Batch = 64, Sigmoid, Adam (lr = 0.0001, num_epoch = 20)\n","\n","# (From 3: 改Sigmoid) 驗證是否是ReLU比較好\n","#### [015/020] Train Acc: 0.771501 Loss: 0.697592 | Val Acc: 0.707262 loss: 0.924157, saving model with acc 0.707\n","\n","\n","---\n","# 6. Val = 0.1, Batch = 64, ReLU, Nadam (lr = 2e-3, num_epoch = 20)\n","# (From 3: 改Nadam)\n","#### [019/020] Train Acc: 0.752201 Loss: 0.856589 | Val Acc: 0.688969 loss: 1.189685, saving model with acc 0.689\n","\n","---\n","# 7. Val = 0.1, Batch = 64, ReLU, Nadam (lr = 0.0001, num_epoch = 20)\n","# (From 6: 改lr)\n","\n","## [Upload: 0.70354]\n","#### [007/020] Train Acc: 0.781690 Loss: 0.650251 | Val Acc: 0.718043 loss: 0.880606, saving model with acc 0.718\n","\n","\n","---\n","# 8. Val = 0.1, Batch = 64, ReLU, Adagrad (lr = 0.01)\n","# (From 3: 改Adagrad, lr)\n","\n","#### [009/020] Train Acc: 0.762591 Loss: 0.722741 | Val Acc: 0.717303 loss: 0.869923, saving model with acc 0.717\n","\n","\n","---\n","# 9. Val = 0.1, Batch = 64, ReLU, Adagrad (lr = 0.05, wd = 0.001)\n","# (From 8: 改lr, wd)\n","\n","#### [017/020] Train Acc: 0.710140 Loss: 0.905975 | Val Acc: 0.700896 loss: 0.937002, saving model with acc 0.701\n","\n","---\n","# 10. Val = 0.1, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.001)\n","# (From 8: 改wd)\n","\n","## [Upload: 0.70433]\n","#### [019/020] Train Acc: 0.749886 Loss: 0.776568 | Val Acc: 0.719409 loss: 0.868873, saving model with acc 0.719\n","\n","---\n","# 11. Val = 0.1, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001)\n","# (From 10: 改Adam)\n","## [Upload: 0.70810]\n","\n","#### [017/020] Train Acc: 0.744398 Loss: 0.779707 | Val Acc: 0.723710 loss: 0.848050, saving model with acc 0.724\n","\n","---\n","# 12. Val = 0.02, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001)\n","# (From 11: 改Val)\n","\n","#### [020/020] Train Acc: 0.746101 Loss: 0.774290 | Val Acc: 0.729095 loss: 0.837620, saving model with acc 0.729\n","\n","---\n","# 13. Val = 0.02, Batch = 64, ReLU, AdaW (lr = 0.0001, wd = 0.01)\n","# (From 12: 改AdamW, 參數預設)\n","\n","#### [006/020] Train Acc: 0.767721 Loss: 0.695459 | Val Acc: 0.725924 loss: 0.843756, saving model with acc 0.726\n","\n","---\n","# 14. Val = 0.02, Batch = 64, ReLU, Adagrad (lr = 0.0001, wd = 0.01)\n","# (From 12: 改Adagrad, 參數預設)\n","\n","#### [018/020] Train Acc: 0.597101 Loss: 1.338628 | Val Acc: 0.615594 loss: 1.303488, saving model with acc 0.616\n","\n","\n","---\n","# 15. Val = 0.02, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001), Dropout = 0.2\n","# (From 12: 加入Dropout)\n","\n","#### [020/020] Train Acc: 0.712875 Loss: 0.889066 | Val Acc: 0.728119 loss: 0.846611, saving model with acc 0.728\n","\n","---\n","# 16. Val = 0.02, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001), Dropout = 0.1, 0.5, 0.1\n","# (From 15: 改Dropout)\n","\n","#### [018/020] Train Acc: 0.679356 Loss: 1.012939 | Val Acc: 0.716208 loss: 0.903012, saving model with acc 0.716\n","\n","---\n","# 17. Val = 0.02, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001),  batch norm\n","# (From 12: 加入batch norm)\n","\n","#### [010/020] Train Acc: 0.758296 Loss: 0.735215 | Val Acc: 0.735314 loss: 0.820954, saving model with acc 0.735\n","\n","---\n","# 18. Val = 0.02, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001),  batch norm, dropout = 0.5 (middile)\n","# (From 16: 加入batch norm, 改Dropout)\n","\n","#### [015/020] Train Acc: 0.705983 Loss: 0.919125 | Val Acc: 0.731412 loss: 0.839901, saving model with acc 0.731\n","\n","---\n","# 19. Val = 0.02, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001), Dropout = 0.5 (middile)\n","# (From 16: 改Dropout)\n","\n","#### [018/020] Train Acc: 0.701185 Loss: 0.929748 | Val Acc: 0.719907 loss: 0.881123, saving model with acc 0.720\n","\n","---\n","# 20. Val = 0.02, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001),  batch norm, epoch = 30\n","# (From 17: 改Epoch)\n","## [Upload: 0.71401]\n","\n","#### [010/030] Train Acc: 0.758296 Loss: 0.735215 | Val Acc: 0.735314 loss: 0.820954, saving model with acc 0.735\n","\n","---\n","# 21. Val = 0.02, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001), Dropout = 0.1 (all)\n","# (From 16: 改Dropout)\n","\n","#### [020/020] Train Acc: 0.712875 Loss: 0.889066 | Val Acc: 0.728119 loss: 0.846611, saving model with acc 0.728\n","\n","---\n","# 22. Val = 0.02, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001),  batch norm, dropout = 0.1 (all)\n","# (From 18: 改dropout)\n","## [Upload: 0.71920]\n","\n","#### [015/020] Train Acc: 0.726784 Loss: 0.845392 | Val Acc: 0.742876 loss: 0.810006, saving model with acc 0.743\n","\n","---\n","# 23. Val = 0.02, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001),  batch norm, dropout = 0.1, 0.5, 0.1\n","# (From 22: 改dropout)\n","\n","#### [018/020] Train Acc: 0.688045 Loss: 0.988221 | Val Acc: 0.730152 loss: 0.853573, saving model with acc 0.730\n","\n","\n","---\n","# 24. Val = 0.02, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 1024, 512, 256, 128, l: 4\n","# (From 22: 加一層, dropout跟著layer)\n","\n","#### [017/020] Train Acc: 0.725571 Loss: 0.851009 | Val Acc: 0.741209 loss: 0.807741, saving model with acc 0.741\n","\n","---\n","# 25. Val = 0.02, Batch = 64, ReLU, Adam (lr = 0.0001, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 24: 改dim)\n","\n","#### [015/020] Train Acc: 0.738183 Loss: 0.801946 | Val Acc: 0.745193 loss: 0.792096, saving model with acc 0.745\n","\n","---\n","# 26. Val = 0.02, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 25: Adagrad)\n","\n","## [Upload: 0.72730]\n","\n","#### [014/020] Train Acc: 0.813472 Loss: 0.558870 | Val Acc: 0.751941 loss: 0.795578, saving model with acc 0.752\n","\n","---\n","\n","# 27. Val = 0.02, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 2048, 1024, 512, 256, 128, l: 5\n","# (From 26: 增加layer)\n","\n","### 跟26差不多（低一點點點）\n","\n","---\n","# 28. Val = 0.02, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.05 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 26: 減少dropout)\n","\n","#### [006/020] Train Acc: 0.773432 Loss: 0.685245 | Val Acc: 0.749095 loss: 0.785736, saving model with acc 0.749\n","\n","---\n","# 29. Val = 0.02, Batch = 32, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 26: 減少batchsize)\n","\n","#### [008/020] Train Acc: 0.748473 Loss: 0.769647 | Val Acc: 0.750315 loss: 0.763709, saving model with acc 0.750\n","\n","---\n","# 30. Val = 0.02, Batch = 16, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 29: 減少batchsize)\n","\n","#### [013/020] Train Acc: 0.730069 Loss: 0.834682 | Val Acc: 0.750030 loss: 0.776581, saving model with acc 0.750\n","\n","---\n","# 31. Val = 0.02, Batch = 128, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 30: 增加batchsize)\n","\n","#### [007/020] Train Acc: 0.783077 Loss: 0.655175 | Val Acc: 0.748608 loss: 0.781434, saving model with acc 0.749\n","\n","---\n","# 32. Val = 0.01, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 26: 減小Val)\n","\n","\n","#### [009/020] Train Acc: 0.783233 Loss: 0.655085 | Val Acc: 0.729268 loss: 0.854641, saving model with acc 0.729\n","\n","---\n","# 33. Val = 0.05, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 26: 增加Val)\n","\n","#### [008/020] Train Acc: 0.776095 Loss: 0.679058 | Val Acc: 0.744492 loss: 0.790004, saving model with acc 0.744\n","\n","---\n","# 34. Val = 0.02, Batch = 64, ReLU, Adadelta (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 26: 換Adadelta)\n","\n","##### 爛到不用記\n","\n","---\n","# 35. Val = 0.02, Batch = 64, ReLU, Adadelta (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 4096, 2048, 1024, 512, 256, l: 5\n","# (From 26: 改layer, dimension)\n","\n","#### [013/020] Train Acc: 0.811250 Loss: 0.567488 | Val Acc: 0.751169 loss: 0.796611, saving model with acc 0.751\n","\n","\n","---\n","# 36. Val = 0.02, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 26: 改dropout)\n","## [Upload: 0.74022]\n","\n","#### [020/020] Train Acc: 0.751780 Loss: 0.771806 | Val Acc: 0.756860 loss: 0.751556, saving model with acc 0.757\n","\n","\n","---\n","# 37. Val = 0.02, Batch = 64, ReLU6, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 26: 改ReLU6)\n","\n","#### [014/020] Train Acc: 0.813472 Loss: 0.558870 | Val Acc: 0.751941 loss: 0.795578, saving model with acc 0.752\n","\n","---\n","# 38. Val = 0.02, Batch = 64, RReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.1 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 36: 改RReLU)\n","\n","#### [020/020] Train Acc: 0.744379 Loss: 0.780384 | Val Acc: 0.750030 loss: 0.766126, saving model with acc 0.750\n","\n","\n","---\n","# 39. Val = 0.02, Batch = 64, RReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 37: 改dropout)\n","\n","#### [020/020] Train Acc: 0.744379 Loss: 0.780384 | Val Acc: 0.750030 loss: 0.766126, saving model with acc 0.750\n","\n","\n","---\n","# 40. Val = 0.02, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 36: 改epoch = 40)\n","## [Upload: 0.74132]\n","\n","#### [030/040] Train Acc: 0.766117 Loss: 0.724972 | Val Acc: 0.760315 loss: 0.750639, saving model with acc 0.760\n","\n","---\n","# 41. Val = 0.02, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.0001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 36: 改wd)\n","\n","#### [020/020] Train Acc: 0.744439 Loss: 0.775144 | Val Acc: 0.754218 loss: 0.748208, saving model with acc 0.754\n","\n","---\n","# 42. Val = 0.02, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.01),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 36: 改wd)\n","\n","#### [018/020] Train Acc: 0.690835 Loss: 1.094645 | Val Acc: 0.738770 loss: 0.916497, saving model with acc 0.739\n","\n","---\n","# 43. Val = 0.02, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.005),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 36: 改wd)\n","\n","#### [018/020] Train Acc: 0.716781 Loss: 0.950057 | Val Acc: 0.746331 loss: 0.829992, saving model with acc 0.746\n","\n","---\n","# 44. Val = 0.02, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.0005),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 36: 改wd)\n","\n","#### [018/020] Train Acc: 0.751767 Loss: 0.761569 | Val Acc: 0.758527 loss: 0.744851, saving model with acc 0.759\n","\n","\n","---\n","# 45. Val = 0.02, Batch = 80, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 36: 改batch)\n","\n","#### [020/020] Train Acc: 0.758614 Loss: 0.749284 | Val Acc: 0.759015 loss: 0.748557, saving model with acc 0.759\n","\n","\n","---\n","# 46. Val = 0.02, Batch = 64, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 4096, 2048, 1024, 512, l: 4\n","# (From 36: 改dimension)\n","\n","#### [018/020] Train Acc: 0.782358 Loss: 0.660127 | Val Acc: 0.758689 loss: 0.752846, saving model with acc 0.759\n","\n","---\n","# 47. Val = 0.02, Batch = 90, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 36: 改batch)\n","\n","#### [020/020] Train Acc: 0.761686 Loss: 0.739953 | Val Acc: 0.758283 loss: 0.746602, saving model with acc 0.758\n","\n","\n","---\n","# 48. Val = 0.02, Batch = 100, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 36: 改batch)\n","\n","#### [016/020] Train Acc: 0.754747 Loss: 0.761855 | Val Acc: 0.755600 loss: 0.757270, saving model with acc 0.756\n","\n","---\n","# 49. Val = 0.02, Batch = 50, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 36: 改batch)\n","\n","#### [017/020] Train Acc: 0.737868 Loss: 0.818511 | Val Acc: 0.754746 loss: 0.758974, saving model with acc 0.755\n","\n","\n","---\n","# 50. Val = 0.02, Batch = 70, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 36: 改batch)\n","\n","#### [018/020] Train Acc: 0.750306 Loss: 0.776669 | Val Acc: 0.755884 loss: 0.773381, saving model with acc 0.756\n","\n","\n","---\n","# 51. Val = 0.02, Batch = 80, ReLU, Adagrad (lr = 0.01, wd = 0.0005),  batch norm (all), dropout = 0.3 (all), d: 4096, 2048, 1024, 512, l: 4\n","# (From 36: 改batch dimension wd)\n","\n","#### [017/020] Train Acc: 0.807262 Loss: 0.568514 | Val Acc: 0.758771 loss: 0.759352, saving model with acc 0.759\n","\n","\n","---\n","# 52. Val = 0.02, Batch = 80, ReLU, Adagrad (lr = 0.01, wd = 0.0005),  batch norm (all), dropout = 0.5 (all), d: 4096, 2048, 1024, 512, l: 4\n","# (From 51: 改dropout)\n","\n","#### [018/020] Train Acc: 0.736598 Loss: 0.814287 | Val Acc: 0.759177 loss: 0.735449, saving model with acc 0.759\n","\n","\n","\n","---\n","# 53. Val = 0.02, Batch = 80, ReLU, Adam (lr = 0.01, wd = 0.0005),  batch norm (all), dropout = 0.3 (all), epoch = 40, d: 4096, 2048, 1024, 512, l: 4\n","# (From 51: 改Adam)\n","\n","#### [004/040] Train Acc: 0.374957 Loss: 2.161849 | Val Acc: 0.439977 loss: 1.928192, saving model with acc 0.440\n","\n","\n","---\n","# 54. Val = 0.02, Batch = 80, ReLU, Adagrad (lr = 0.01, wd = 0.0005),  batch norm (all), dropout = 0.1 (all), d: 4096, 2048, 1024, 512, l: 4\n","# (From 51: 改dropout)\n","\n","#### [004/020] Train Acc: 0.753637 Loss: 0.739950 | Val Acc: 0.747469 loss: 0.773155, saving model with acc 0.747\n","\n","\n","---\n","# 55. Val = 0.02, Batch = 32, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 36: 改epoch = 1000, es=60)\n","## [Upload: 0.74476]\n","\n","#### [069/1000] Train Acc: 0.759293 Loss: 0.745836 | Val Acc: 0.765885 loss: 0.725001, saving model with acc 0.766\n","\n","---\n","# 56. Val = 0.05, Batch = 32, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 55: Val改0.05)\n","\n","\n","---\n","# 57. Val = 0.02, Batch = 50, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 55: Batch改50)\n","\n","#### [125/1000] Train Acc: 0.798755 Loss: 0.619928 | Val Acc: 0.766251 loss: 0.754857, saving model with acc 0.766\n","\n","---\n","# 58. Val = 0.05, Batch = 50, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, l: 4\n","# (From 56: Batch改50)\n","\n","#### [046/1000] Train Acc: 0.771962 Loss: 0.706526 | Val Acc: 0.757679 loss: 0.760074, saving model with acc 0.758\n","---\n","# 59. Val = 0.02, Batch = 50, ReLU, Adagrad (lr = 0.01, wd = 0.001),  batch norm (all), dropout = 0.3 (all), d: 2048, 1024, 512, 256, 128, 64,l: 6\n","# (From 57: l改6)\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"SHARE MLSpring2021 - HW2-1-2.ipynb","provenance":[{"file_id":"https://github.com/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb","timestamp":1615575419085}],"toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}